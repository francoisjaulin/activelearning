{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "daticu_al_for_hypertension.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francoisjaulin/activelearning/blob/master/daticu_al_for_hypertension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9Vl9FKW0R8BC",
        "colab_type": "code",
        "outputId": "ebe7573f-a8ca-4723-9f83-2b57e9d48919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%set_env GOOGLE_CLOUD_PROJECT chc-mimic-analysis\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!touch ~/.bigqueryrc\n",
        "\n",
        "# Silence some log spam from this library.`\n",
        "import logging\n",
        "logging.getLogger('googleapiclient').setLevel(logging.CRITICAL)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: GOOGLE_CLOUD_PROJECT=chc-mimic-analysis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C4oXk1O8eJpN",
        "colab_type": "code",
        "outputId": "882e97f6-f2ca-4216-9cfa-a70dd594b993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install sklearn-pandas\n",
        "\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-pandas\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/9c/c94f46b40b86d2c77c46c4c1b858fc66c117b4390665eca28f2e0812db45/sklearn_pandas-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (0.19.1)\n",
            "Requirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (0.19.2)\n",
            "Requirement already satisfied: pandas>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.11.0->sklearn-pandas) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.11.0->sklearn-pandas) (2018.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.11.0->sklearn-pandas) (1.11.0)\n",
            "Installing collected packages: sklearn-pandas\n",
            "Successfully installed sklearn-pandas-1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zHnZTd5sq_AH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#max_id = 62066420\n",
        "max_id = 62096420\n",
        "#max_id = 162096420"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yFWdxkNvcE0-",
        "colab_type": "code",
        "outputId": "8af70397-f14b-4f88-f540-9f5853b90fc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1452
        }
      },
      "cell_type": "code",
      "source": [
        "# Query to obtain the labels\n",
        "query = '''\n",
        "SELECT\n",
        "  ARRAY_AGG(value_as_string) as values,\n",
        "  person_id,\n",
        "  visit_occurrence_id\n",
        "FROM\n",
        "  `chc-mimic-data.mimic3_aphp.observation`  AS t\n",
        "WHERE\n",
        "  t.observation_source_concept_id='2001028679'\n",
        "  AND person_id < %d\n",
        "GROUP BY\n",
        "  person_id,\n",
        "  visit_occurrence_id\n",
        "'''\n",
        "\n",
        "query = (query) % max_id\n",
        "\n",
        "df = pd.read_gbq(query, project_id='chc-mimic-analysis', verbose=False, dialect='standard')\n",
        "df.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=495642085510-k0tmvj2m941jhre2nbqka17vqpjfddtd.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&state=amJ4TTyn5oAiyjpkl8NH6XhYmcHUZi&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/VQCShPCKmHkQTe29-hsxZgZ7ySuRyBexKCFkaiHmIXwt8ZNTdadstk4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "GenericGBQException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 job_config=_query.query_config(\n\u001b[0;32m--> 488\u001b[0;31m                     job_config, BIGQUERY_INSTALLED_VERSION))\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ok.\\nQuery running...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             job_ref, query, client=self, job_config=job_config)\n\u001b[0;32m-> 1080\u001b[0;31m         \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             method='POST', path=path, data=self._build_resource())\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 POST https://www.googleapis.com/bigquery/v2/projects/chc-mimic-analysis/jobs: Access Denied: Project chc-mimic-analysis: The user FrancoisJaulin@gmail.com does not have bigquery.jobs.create permission in project chc-mimic-analysis.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-225ca16809cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_gbq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'chc-mimic-analysis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'standard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, verbose, private_key, dialect, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mprivate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, verbose, private_key, auth_local_webserver, dialect, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         dialect=dialect, auth_local_webserver=auth_local_webserver)\n\u001b[0;32m--> 821\u001b[0;31m     \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m     \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m                     \"please re-run the application to re-authorize\")\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_reply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mprocess_http_error\u001b[0;34m(ex)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# <https://cloud.google.com/bigquery/troubleshooting-errors>`__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mGenericGBQException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reason: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGenericGBQException\u001b[0m: Reason: 403 POST https://www.googleapis.com/bigquery/v2/projects/chc-mimic-analysis/jobs: Access Denied: Project chc-mimic-analysis: The user FrancoisJaulin@gmail.com does not have bigquery.jobs.create permission in project chc-mimic-analysis."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "PAZxcaPuepPD",
        "colab_type": "code",
        "outputId": "6bb762aa-93c9-4d67-c3e4-13c02ccd5ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "cell_type": "code",
      "source": [
        "label_name = 'Hypertension'\n",
        "\n",
        "labels = {}\n",
        "\n",
        "positives = 0\n",
        "total = 0\n",
        "for index, row in df.iterrows():\n",
        "  key = str(row[\"person_id\"]) + \":\" + str(row[\"visit_occurrence_id\"])\n",
        "  label_values = row[\"values\"]\n",
        "  conditions = []\n",
        "  for v in label_values:\n",
        "    conditions.append(v['v'])\n",
        "  label_value = (label_name in conditions)\n",
        "  if label_value:\n",
        "    positives = positives + 1\n",
        "  total = total + 1\n",
        "  labels[key] = label_value\n",
        "\n",
        "print ('positives %d' % positives)\n",
        "print ('total %d' % total)\n",
        "print (labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8aa74c7abbe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpositives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"person_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"visit_occurrence_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mlabel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ERdnu-cFSiPw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select the discharge summary medical history\n",
        "query = 'SELECT nlp.note_id,nlp.lexical_variant,note.person_id,note.visit_occurrence_id FROM `chc-mimic-data.mimic3_aphp.note_nlp` as nlp inner join `chc-mimic-data.mimic3_aphp.note` as note ON nlp.note_id = note.note_id WHERE nlp.lexical_variant LIKE \"HIST%\" AND note.person_id < {maxid}'.format(maxid=max_id)\n",
        "df = pd.read_gbq(query, project_id='chc-mimic-analysis', verbose=False, dialect='standard')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lep0iCJjVhIy",
        "colab_type": "code",
        "outputId": "4a116121-e986-40ef-cfcb-7b4431c2afdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "# Dump some to see what it looks like\n",
        "print (len(df['note_id']))\n",
        "df['key'] = df['person_id'].astype(str) + ':' + df['visit_occurrence_id'].astype(str)\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ea758d5356ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'note_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'visit_occurrence_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "WNflUHFAcUcM",
        "colab_type": "code",
        "outputId": "d008c9b6-d317-4f09-d6b5-25e4b9c41bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "coarse_label = labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3c430c656e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoarse_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0Zt1dzzAudDM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Print the top tokens having the most influence\n",
        "# when the model considers a particular item\n",
        "# ie, the largest (by absolute value) products of scalars\n",
        "# that compose the dot product in the linear model\n",
        "def print_top_tokens_decision(model, item_weights, N=10):\n",
        "  import operator\n",
        "  \n",
        "  model_coefs = model.coef_[0]\n",
        "  \n",
        "  coefs = [ abs(model_coefs*item_weights) for model_coefs, item_weights in zip(model_coefs, item_weights) ]\n",
        "  \n",
        "  index_coef = sorted( enumerate(coefs), key=operator.itemgetter(1))\n",
        "  model_size = len(index_coef)\n",
        "  print (index_coef[-1])\n",
        "  index_token = { index:token for token, index in vectorizer.vocabulary_.items() }\n",
        "  largest_index_list, largest_coef_list = zip( *index_coef )\n",
        "  largest_index_list = largest_index_list[-N:]\n",
        "  # print (largest_index_list)\n",
        "  # print( largest_coef_list[-N:])\n",
        "  print ('Top {N} tokens (for specific item):'.format(N=N))\n",
        "  print ([index_token[index] for index in largest_index_list])\n",
        "  print ([coefs[index] for index in largest_index_list])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fzaEqBoaUL6B",
        "colab_type": "code",
        "outputId": "f8ce6ec8-bdba-4360-e53a-fe517ab8ab0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create the feacture vector for the notes.\n",
        "NGRAM_MAX = 3\n",
        "vectorizer = CountVectorizer(min_df=10, stop_words='english', ngram_range=(1,NGRAM_MAX))\n",
        "\n",
        "tfidf_mapper = DataFrameMapper([\n",
        "   ('lexical_variant', [vectorizer,TfidfTransformer()] )\n",
        "   ])\n",
        "tfidf = tfidf_mapper.fit_transform(df)\n",
        "\n",
        "\n",
        "# Make the labels.\n",
        "label_manual = {\n",
        "    \"62064720:None\" : 0,\n",
        "    \"62064225:None\" : 0,\n",
        "    \"62065246:1331\" : 1,\n",
        "    \"62064225:1792\" : 1,\n",
        "    \"62063486:1218\" : 1,\n",
        "    \n",
        "               }\n",
        "label = dict( list(coarse_label.items()) + list(label_manual.items()) )\n",
        "\n",
        "count = len(df.note_id)\n",
        "\n",
        "labels = np.zeros(count)\n",
        "\n",
        "# Make the labels from the positives found.\n",
        "for i in range(0, count):\n",
        " nid = df.key[i]\n",
        " if nid in label:\n",
        "   labels[i] = label[nid]\n",
        "  \n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train a model for the active learning.\n",
        "#model = SGDClassifier(loss='hinge', penalty='none',\n",
        "#                     alpha=0.00, random_state=42,\n",
        "#                     learning_rate='constant',\n",
        "#                     eta0=1.0, fit_intercept=False, class_weight='balanced')\n",
        "model = LogisticRegression(max_iter=1, fit_intercept=False)\n",
        "#model = RandomForestClassifier(max_depth=8, min_samples_leaf=2)\n",
        "model.fit(tfidf, labels)\n",
        "predictions = model.predict_proba(tfidf)\n",
        "\n",
        "def plot_histogram(values, title):\n",
        "  \n",
        "  import matplotlib.mlab as mlab\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # the histogram of the data\n",
        "  n, bins, patches = plt.hist(values, 50, normed=1, facecolor='green', alpha=0.75)\n",
        "\n",
        "  # plt.xlabel('Smarts')\n",
        "  # plt.ylabel('Probability')\n",
        "  plt.title( title )\n",
        "  # plt.axis([40, 160, 0, 0.03])\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_histogram( predictions, 'predictions' )\n",
        "  \n",
        "# Print the top tokens\n",
        "def print_top_tokens_model(model, N=10):\n",
        "  import operator\n",
        "  coefs = model.coef_[0]\n",
        "  index_coef = sorted( enumerate(coefs), key=operator.itemgetter(1))\n",
        "  model_size = len(index_coef)\n",
        "  print (index_coef[-1])\n",
        "  index_token = { index:token for token, index in vectorizer.vocabulary_.items() }\n",
        "  largest_index_list, largest_coef_list = zip( *index_coef )\n",
        "  largest_index_list = largest_index_list[-N:]\n",
        "  # print (largest_index_list)\n",
        "  # print( largest_coef_list[-N:])\n",
        "  print ('Top {N} tokens (overall):'.format(N=N))\n",
        "  print ([index_token[index] for index in largest_index_list])\n",
        "  \n",
        "print_top_tokens_model(model, N=10) \n",
        "\n",
        "  \n",
        "# Find the next sample for active learning\n",
        "\n",
        "policy = \"uncertain\"\n",
        "#policy = \"largest\"\n",
        "\n",
        "if policy == \"uncertain\":\n",
        "  candidate = -1\n",
        "  candidate_score = 100\n",
        "  for i in range(0, count):\n",
        "    if df.key[i] not in label:\n",
        "       score = abs(predictions[i][0] - 0.5)\n",
        "       if score < candidate_score:\n",
        "         candidate_score = score\n",
        "         candidate = i\n",
        "        \n",
        "if policy == \"largest\":\n",
        "  candidate = -1\n",
        "  candidate_score = 0\n",
        "  for i in range(0, count):\n",
        "    if df.key[i] not in label:\n",
        "       score = predictions[i][0]\n",
        "       if score > candidate_score:\n",
        "         candidate_score = score\n",
        "         candidate = i\n",
        "      \n",
        "#print (labels[:10])\n",
        "#print (predictions[:10])\n",
        "print ('next candidate is key %s note_id %d with score %s' % (df.key[candidate],\n",
        "                                                              df.note_id[candidate],\n",
        "                                                              candidate_score))\n",
        "\n",
        "print_top_tokens_decision(model, tfidf[candidate], N=10)\n",
        "print (df.lexical_variant[candidate])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8f1e65d90088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn_pandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameMapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_projection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianRandomProjection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_pandas'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sPjr7kErsFsY",
        "colab_type": "code",
        "outputId": "8f13623e-5aa9-48f8-c26d-9e8b1a7a6068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "cell_type": "code",
      "source": [
        "x = tfidf[candidate]\n",
        "print(x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c48390430f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "25XmNYGDzXS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}